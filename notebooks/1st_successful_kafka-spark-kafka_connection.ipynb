{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"OpenSEA Pipeline\")\n",
    "    .config(\"spark.streaming.stopGracefullyonShutdown\", True)\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data from kafka topic\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"os-kafka:9092\")\n",
    "    .option(\"subscribe\", \"test-os_event\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show kafka topic schema and messages\n",
    "\n",
    "kafka_df.printSchema()\n",
    "# kafka_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse value from binary to string into kafka_json_df\n",
    "from pyspark.sql.functions import expr, schema_of_json\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka_json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block to retrieve schema from json data\n",
    "# first make sure spark is in '.read' mode and not '.readStream' mode\n",
    "\n",
    "# json_schema = schema_of_json(kafka_json_df.select('value').head()[0])\n",
    "# json_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType\n",
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"payload\", StructType([\n",
    "        StructField(\"collection\", StructType([\n",
    "            StructField(\"slug\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"event_timestamp\", StringType(), True),\n",
    "        StructField(\"item\", StructType([\n",
    "            StructField(\"chain\", StructType([\n",
    "                StructField(\"name\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"metadata\", StructType([\n",
    "                StructField(\"image_url\", StringType(), True),\n",
    "                StructField(\"metadata_url\", StringType(), True),\n",
    "                StructField(\"name\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"nft_id\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"maker\", StructType([\n",
    "            StructField(\"address\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"order_hash\", StringType(), True),\n",
    "        StructField(\"payment_token\", StructType([\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"decimals\", LongType(), True),\n",
    "            StructField(\"eth_price\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"symbol\", StringType(), True),\n",
    "            StructField(\"usd_price\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"protocol_address\", StringType(), True),\n",
    "        StructField(\"protocol_data\", StructType([\n",
    "            StructField(\"parameters\", StructType([\n",
    "                StructField(\"conduitKey\", StringType(), True),\n",
    "                StructField(\"consideration\", ArrayType(StructType([\n",
    "                    StructField(\"endAmount\", StringType(), True),\n",
    "                    StructField(\"identifierOrCriteria\", StringType(), True),\n",
    "                    StructField(\"itemType\", LongType(), True),\n",
    "                    StructField(\"recipient\", StringType(), True),\n",
    "                    StructField(\"startAmount\", StringType(), True),\n",
    "                    StructField(\"token\", StringType(), True)\n",
    "                ])), True),\n",
    "                StructField(\"counter\", StringType(), True),\n",
    "                StructField(\"endTime\", StringType(), True),\n",
    "                StructField(\"offer\", ArrayType(StructType([\n",
    "                    StructField(\"endAmount\", StringType(), True),\n",
    "                    StructField(\"identifierOrCriteria\", StringType(), True),\n",
    "                    StructField(\"itemType\", LongType(), True),\n",
    "                    StructField(\"startAmount\", StringType(), True),\n",
    "                    StructField(\"token\", StringType(), True)\n",
    "                ])), True),\n",
    "                StructField(\"offerer\", StringType(), True),\n",
    "                StructField(\"orderType\", LongType(), True),\n",
    "                StructField(\"salt\", StringType(), True),\n",
    "                StructField(\"startTime\", StringType(), True),\n",
    "                StructField(\"totalOriginalConsiderationItems\", LongType(), True),\n",
    "                StructField(\"zone\", StringType(), True),\n",
    "                StructField(\"zoneHash\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"signature\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"quantity\", LongType(), True),\n",
    "        StructField(\"sale_price\", StringType(), True),\n",
    "        StructField(\"taker\", StructType([\n",
    "            StructField(\"address\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"transaction\", StructType([\n",
    "            StructField(\"hash\", StringType(), True),\n",
    "            StructField(\"timestamp\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True),\n",
    "    StructField(\"sent_at\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json,col\n",
    "\n",
    "raw_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema)).selectExpr(\"values_json.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.printSchema()\n",
    "# raw_df.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.createDataFrame([('name', 'val')], schema='key string, value string')\n",
    "# df_test.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_os_df = kafka_json_df.withColumn(\n",
    "    \"values_json\", \\\n",
    "    from_json(col(\"value\"), \\\n",
    "    json_schema)).selectExpr(\n",
    "    \"values_json.payload.collection.slug as collection\",\n",
    "    \"values_json.payload.item.metadata.image_url\",) \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"os-kafka:9095\") \\\n",
    "    .option(\"topic\", \"test-topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kafka_topic_text_data (silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_df = kafka_json_df.withColumn(\n",
    "    \"values_json\", \\\n",
    "    from_json(col(\"value\"), \\\n",
    "    json_schema)).selectExpr(\n",
    "    \"values_json.payload.collection.slug as collection\", \\\n",
    "    \"values_json.payload.item.metadata.name as nft_name\", \\\n",
    "    \"values_json.payload.item.nft_id\", \\\n",
    "    \"values_json.payload.transaction.hash as transaction_hash\", \\\n",
    "    \"values_json.payload.transaction.timestamp as transaction_timestamp\",) \\\n",
    "    # .writeStream \\\n",
    "    # .format(\"kafka\") \\\n",
    "    # .option(\"kafka.bootstrap.servers\", \"os-kafka:9095\") \\\n",
    "    # .option(\"topic\", \"test-topic\") \\\n",
    "    # .option(\"checkpointLocation\", \"checkpoint_dir_kafka\") \\\n",
    "    # .start()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "text_data (silver)\n",
    " |-- payload: struct (nullable = true)\n",
    " |    |-- collection: struct (nullable = true)\n",
    " |    |    |-- slug: string (nullable = true)\n",
    " |    |-- item: struct (nullable = true)\n",
    " |    |    |-- metadata: struct (nullable = true)\n",
    " |    |    |    |-- name: string (nullable = true)\n",
    " |    |    |-- nft_id: string (nullable = true)\n",
    " |    |-- transaction: struct (nullable = true)\n",
    " |    |    |-- hash: string (nullable = true)\n",
    " |    |    |-- timestamp: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_topic_3.printSchema()\n",
    "# kafka_topic_2.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct, to_json, lit\n",
    "\n",
    "# Convert the DataFrame to the required format for Kafka\n",
    "kafka_ready_df = text_data_df.withColumn(\n",
    "    \"value\", to_json(struct(\n",
    "        col(\"collection\"),\n",
    "        col(\"nft_name\"),\n",
    "        col(\"nft_id\"),\n",
    "        col(\"transaction_hash\"),\n",
    "        col(\"transaction_timestamp\")\n",
    "    ))\n",
    ").withColumn(\n",
    "    \"key\", lit(None).cast(\"string\")  # Optional: Set to None if not using a key\n",
    ").select(\"key\", \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(kafka_ready_df.writeStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"os-kafka:9092\") \\\n",
    "                .option(\"topic\", \"test-topic\") \\\n",
    "                .option(\"checkpointLocation\", \"checkpoint_dir_kafka\") \\\n",
    "                .start())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
